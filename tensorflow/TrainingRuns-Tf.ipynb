{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: right\"> &uarr;   Ensure Kernel is set to  &uarr;  </div><br><div style=\"text-align: right\"> conda_amazonei_tensorflow2_p36  </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensorflow Estimator Bring your own Script\n",
    "\n",
    "In this notebook we will go through and run a tensorflow model to classify the junctions as priority, signal and roundabout as seen in data prep.\n",
    "\n",
    "The outline of this notebook is \n",
    "\n",
    "1. To prepare a training script (provided).\n",
    "\n",
    "2. Use the AWS provided Tensorflow container and provide our script to it.\n",
    "\n",
    "3. Run training.\n",
    "\n",
    "4. Deploy model to end point.\n",
    "\n",
    "5. Test using an image in couple of possible ways "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upgrade Sagemaker so we can access the latest containers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U 'sagemaker>=2.48'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us also upgrade out version of Tensorflow to v2.4.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tensorflow==2.4.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets make sure that our environment is using Tensorflow 2.4.1 otherwise we will need to restart the notebook kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "print(f\"Tensorflow version {tf.__version__}\")\n",
    "\n",
    "if tf.__version__ != \"2.4.1\":\n",
    "    print(\"This notebook kernel needs to be restarted!!!!\")\n",
    "    exit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets start by importing some libraries that we will be using later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sagemaker\n",
    "import numpy as np\n",
    "from sagemaker.tensorflow import TensorFlow\n",
    "# if you are using pytorch\n",
    "# from sagemaker.pytorch import PyTorch\n",
    "\n",
    "\n",
    "ON_SAGEMAKER_NOTEBOOK = True\n",
    "\n",
    "sagemaker_session = sagemaker.Session()\n",
    "if ON_SAGEMAKER_NOTEBOOK:\n",
    "    role = sagemaker.get_execution_role()\n",
    "else:\n",
    "    role = \"[YOUR ROLE]\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A quick sanity check to make sure we are using the latest version of SageMaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sagemaker.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Input params for model training \n",
    "\n",
    "In the cell below, replace **\"your-unique-bucket-name\"** with the name of bucket you created in the data-prep notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket = \"your-unique-bucket-name\"\n",
    "key = \"\"                            # Path from the bucket's root to the dataset\n",
    "\n",
    "\n",
    "train_instance_type='ml.m5.12xlarge'      # The type of EC2 instance which will be used for training\n",
    "deploy_instance_type='ml.m5.4xlarge'     # The type of EC2 instance which will be used for deployment\n",
    "\n",
    "'''\n",
    "we can use the train and validation path as stated above \n",
    "or you can \n",
    "just rearrange data and use a single path like below\n",
    "'''\n",
    "training_data_uri=\"s3://{}\".format(bucket)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensorflow Estimator\n",
    "\n",
    "Use AWS provided open source containers, these containers can be extended by starting with the image provided by AWS and the add additional installs in dockerfile\n",
    "\n",
    "or you can use requirements.txt in source_dir to install additional libraries.\n",
    "\n",
    "We setup the Tensorflow estimator job a job name, an entry point (which is our script **tfModelCode.py**), role, Tensorflow framework version, python version, instance count and type. <br>\n",
    "Then we call the estimators fit method with the URI of the training dataset to kick off the training job.<br>\n",
    "**Note: This cell will take approx 30 mins to complete**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "estimator_tf = TensorFlow(\n",
    "  base_job_name='tensorflow-pssummit-traffic-class',\n",
    "  entry_point=\"tfModelCode.py\",             # Your entry script\n",
    "  role=role,\n",
    "  framework_version=\"2.4.1\",               # TensorFlow's version\n",
    "  py_version=\"py37\",\n",
    "  instance_count=1,  # \"The number of GPUs instances to use\"\n",
    "  instance_type=train_instance_type,\n",
    ")\n",
    "\n",
    "print(\"Training ...\")\n",
    "estimator_tf.fit(training_data_uri)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **NOTE:** <br>\n",
    "If at this point your kernel disconnects from the server (you can tell because the kernel in the top right hand corner will say **No Kernel**, you can reattach to the training job (so you dont to start the training job again.<br>Follow the steps below\n",
    "1. Scoll your notebook to the top and set the kernel to the recommended kernel specified in the top right hand corner of the notebook\n",
    "2. Go to your SageMaker console, Go to Training Jobs and copy the name of the training job you were disconnected from\n",
    "3. Scoll to the bottom of this notebook, paste your training job name to replace the **your-training-job-name** in the cell\n",
    "4. Run the edited cell\n",
    "5. Return to this cell and continue executing the rest of this notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Deploying a model\n",
    "Once trained, deploying a model is a simple call. <br>\n",
    "We specify two prarameters<br>\n",
    "    **instance_type** - the type of the instance will be used to do inference<br>\n",
    "    **initial_instance_count** - the initial number of instances that will be provisioned to do inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator_deployed=estimator_tf.deploy(instance_type='ml.m5.2xlarge', initial_instance_count=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the estimator has been deployed to an endpoint, lets find out the endpoint name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(estimator_deployed.endpoint_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So to do predictions againast this endpoint, we are going to use Predictor. We provide it the endpoint name, the SageMaker session and the serializer (in our case a JSONSerializer)\n",
    "Serializers implement methods for serializing data for an inference endpoint<br>\n",
    "**NOTE** Replace **'your-endpoint-name'** with your endpoint name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.predictor import Predictor\n",
    "from sagemaker.serializers import JSONSerializer\n",
    "\n",
    "endpoint_name = 'your-endpoint-name'\n",
    "\n",
    "predictor=Predictor(endpoint_name=endpoint_name,\n",
    "                    sagemaker_session=sagemaker_session, \n",
    "                    serializer=JSONSerializer())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we install some convenience libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will take one of our test images and apply some preprocessing to it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file='../data/test/Roundabout/R2.png'\n",
    "img = tf.keras.preprocessing.image.load_img(file, target_size=[250, 250])\n",
    "plt.imshow(img)\n",
    "plt.axis('off')\n",
    "x = tf.keras.preprocessing.image.img_to_array(img)\n",
    "x = tf.keras.applications.efficientnet.preprocess_input(\n",
    "    x[tf.newaxis,...])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we send that processed data to our endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.predict(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see the prediction has sent back a confidence score for each class. The second value in the list corresponds to the class label \"Roundabout\" which has the highest confidence score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using boto3 sagemaker_runtime client\n",
    "\n",
    "So what if we want to make predictions against this endpoint outside of this notebook?<br>\n",
    "We then leverage the boto3 library. <br>\n",
    "**NOTE** Replace **'your-endpoint-name'** with your endpoint name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "client=boto3.client('sagemaker-runtime')\n",
    "response = client.invoke_endpoint(\n",
    "            EndpointName='your-endpoint-name',\n",
    "            ContentType='application/json',\n",
    "            Body=json.dumps({'instances':x.tolist()}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now view the JSON response. Again the second value in the list corresponds to the class label \"Roundabout\" which has the highest confidence score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json.loads(response['Body'].read().decode(\"utf-8\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean up\n",
    "\n",
    "When we're done with the endpoint, we can just delete it and the backing instances will be released.  Run the following cell to delete the endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.delete_endpoint()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will start by creating a S3 URI to the model artifacts package generated from the training step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_data = f\"{estimator_tf.output_path}{estimator_tf._current_job_name}/output/model.tar.gz\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First let us make sure we are in the correct starting folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd ~/SageMaker/pssummitwkshp/tensorflow/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will create a local export folder so we can store our inference code in a code folder. We can also specify a requirements.txt for any package dependencies  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! mkdir ./export"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we copy and unpack the model artifacts file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws s3 cp {model_data} ./export/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tar -xvzf ./export/model.tar.gz -C ./export/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd export"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now delete any old model artifacts folders and move the unpacked model artifacts folder to the 1 folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! rm -r 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! mv tf000000001/1 ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! rm -r code/.ipynb_checkpoints/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now package up the code and 1 folder to create a new model.tar.gz file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! tar -czvf model.tar.gz code 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We copy the new model.tar.gz to your S3 bucket and setup our Tensorflow Serving Container https://github.com/aws/sagemaker-tensorflow-serving-container"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sagemaker\n",
    "from sagemaker.tensorflow.model import TensorFlowModel\n",
    "sm_role=sagemaker.get_execution_role()\n",
    "sagemaker_session = sagemaker.Session()\n",
    "# See the following document for more on SageMaker Roles:\n",
    "# https://docs.aws.amazon.com/sagemaker/latest/dg/sagemaker-roles.html\n",
    "role = sm_role\n",
    "\n",
    "# Will be using the bucket variable defined at beginning of this notebook\n",
    "\n",
    "prefix = 'tf_model'\n",
    "s3_path = 's3://{}/{}'.format(bucket, prefix)\n",
    "\n",
    "model_data = sagemaker_session.upload_data('model.tar.gz',\n",
    "                                           bucket,\n",
    "                                           os.path.join(prefix, 'model'))\n",
    "                                           \n",
    "tensorflow_serving_model = TensorFlowModel(model_data=model_data,\n",
    "                                 role=role,\n",
    "                                 framework_version='2.4.1',\n",
    "                                 sagemaker_session=sagemaker_session)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then specify the output folder and run the transformer method to start the batch processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path = f's3://{bucket}/{prefix}/output'\n",
    "tensorflow_serving_transformer = tensorflow_serving_model.transformer(\n",
    "                                     instance_count=2,\n",
    "                                     instance_type='ml.m5.4xlarge',\n",
    "                                     max_concurrent_transforms=64,\n",
    "                                     max_payload=1,\n",
    "                                     output_path=output_path)\n",
    "\n",
    "input_path = f's3://{bucket}/test'\n",
    "tensorflow_serving_transformer.transform(input_path, content_type='application/x-image')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output of this batch job will be in the following S3 URI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can look at the output file from the batch job. Each file is a prediction that corresponds to the input image file name "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! aws s3 ls {output_path} --recursive | grep -v \".ipy\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attach to a training job that has been left to run \n",
    "\n",
    "If your kernel becomes disconnected and your training has already started, you can reattach to the training job.<br>\n",
    "In the cell below, replace **your-unique-bucket-name** with the name of bucket you created in the data-prep notebook<br>\n",
    "Then look up the training job name and replace the **your-training-job-name** and then run the cell below. <br>\n",
    "Once the training job is finished, you can continue the cells after the training cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import sagemaker\n",
    "import boto3\n",
    "\n",
    "sess = sagemaker.Session()\n",
    "role = sagemaker.get_execution_role()\n",
    "\n",
    "bucket = \"your-unique-bucket-name\"\n",
    "\n",
    "training_job_name = 'your-training-job-name'\n",
    "\n",
    "if 'your-training' not in training_job_name:\n",
    "    estimator_tf = sagemaker.estimator.Estimator.attach(training_job_name=training_job_name, sagemaker_session=sess)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_amazonei_tensorflow2_p36",
   "language": "python",
   "name": "conda_amazonei_tensorflow2_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
