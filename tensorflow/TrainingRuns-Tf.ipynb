{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensorflow Estimator Bring your own Script\n",
    "\n",
    "In this notebook we will go through and run a tensorflow model to classify the junctions as priority, signal and roundabout as seen in data prep.\n",
    "\n",
    "The outline of this notebook is \n",
    "\n",
    "1. To prepare a training script (provided).\n",
    "\n",
    "2. Use the AWS provided Tensorflow container and provide our script to it.\n",
    "\n",
    "3. Run training.\n",
    "\n",
    "4. Deploy model to end point.\n",
    "\n",
    "5. Test using an image in couple of possible ways "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upgrade Sagemaker so we can access the latest containers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "sttime = datetime.datetime.now()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U sagemaker>=2.48"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us also upgrade out version of Tensorflow to v2.4.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tensorflow==2.4.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets make sure that our environment is using Tensorflow 2.4.1 otherwise we will need to restart the notebook kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "print(f\"Tensorflow version {tf.__version__}\")\n",
    "\n",
    "if tf.__version__ != \"2.4.1\":\n",
    "    print(\"This notebook kernel needs to be restarted!!!!\")\n",
    "    exit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets start by importing some libraries that we will be using later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sagemaker\n",
    "import numpy as np\n",
    "from sagemaker.tensorflow import TensorFlow\n",
    "# if you are using pytorch\n",
    "# from sagemaker.pytorch import PyTorch\n",
    "\n",
    "\n",
    "ON_SAGEMAKER_NOTEBOOK = True\n",
    "\n",
    "sagemaker_session = sagemaker.Session()\n",
    "if ON_SAGEMAKER_NOTEBOOK:\n",
    "    role = sagemaker.get_execution_role()\n",
    "else:\n",
    "    role = \"[YOUR ROLE]\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A quick sanity check to make sure we are using the latest version of SageMaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sagemaker.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Input params for model training \n",
    "\n",
    "In the cell below, replace **\"your-unique-bucket-name\"** with the name of bucket you created in the data-prep notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket = \"your-unique-bucket-name\"\n",
    "key = \"\"                            # Path from the bucket's root to the dataset\n",
    "\n",
    "\n",
    "train_instance_type='ml.m5.12xlarge'      # The type of EC2 instance which will be used for training\n",
    "deploy_instance_type='ml.m5.4xlarge'     # The type of EC2 instance which will be used for deployment\n",
    "\n",
    "'''\n",
    "we can use the train and validation path as stated above \n",
    "or you can \n",
    "just rearrange data and use a single path like below\n",
    "'''\n",
    "training_data_uri=\"s3://{}\".format(bucket)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensorflow Estimator\n",
    "\n",
    "Use AWS provided open source containers, these containers can be extended by starting with the image provided by AWS and the add additional installs in dockerfile\n",
    "\n",
    "or you can use requirements.txt in source_dir to install additional libraries.\n",
    "\n",
    "We setup the Tensorflow estimator job a job name, an entry point (which is our script **tfModelCode.py**), role, Tensorflow framework version, python version, instance count and type. <br>\n",
    "Then we call the estimators fit method with the URI of the training dataset to kick off the training job.<br>\n",
    "**Note: This cell will take approx 40-50 mins to complete**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "estimator_tf = TensorFlow(\n",
    "  base_job_name='tensorflow-pssummit-traffic-class',\n",
    "  entry_point=\"tfModelCode.py\",             # Your entry script\n",
    "  role=role,\n",
    "  framework_version=\"2.4.1\",               # TensorFlow's version\n",
    "  py_version=\"py37\",\n",
    "  instance_count=1,  # \"The number of GPUs instances to use\"\n",
    "  instance_type=train_instance_type,\n",
    ")\n",
    "\n",
    "print(\"Training ...\")\n",
    "estimator_tf.fit(training_data_uri)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Deploying a model\n",
    "Once trained, deploying a model is a simple call. <br>\n",
    "We specify two prarameters<br>\n",
    "    **instance_type** - the type of the instance will be used to do inference<br>\n",
    "    **initial_instance_count** - the initial number of instances that will be provisioned to do inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator_deployed=estimator_tf.deploy(instance_type='ml.m5.2xlarge', initial_instance_count=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the estimator has been deployed to an endpoint, lets find out the endpoint name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(estimator_deployed.endpoint_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So to do predictions againast this endpoint, we are going to use Predictor. We provide it the endpoint name, the SageMaker session and the serializer (in our case a JSONSerializer)\n",
    "Serializers implement methods for serializing data for an inference endpoint<br>\n",
    "**NOTE** Replace **'your-endpoint-name'** with your endpoint name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.predictor import Predictor\n",
    "from sagemaker.serializers import JSONSerializer\n",
    "\n",
    "endpoint_name = 'your-endpoint-name'\n",
    "\n",
    "predictor=Predictor(endpoint_name=endpoint_name,\n",
    "                    sagemaker_session=sagemaker_session, \n",
    "                    serializer=JSONSerializer())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we install some convenience libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vars(tf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will take one of our test images and apply some preprocessing to it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file='../data/test/Roundabout/R2.png'\n",
    "img = tf.keras.preprocessing.image.load_img(file, target_size=[250, 250])\n",
    "plt.imshow(img)\n",
    "plt.axis('off')\n",
    "x = tf.keras.preprocessing.image.img_to_array(img)\n",
    "x = tf.keras.applications.efficientnet.preprocess_input(\n",
    "    x[tf.newaxis,...])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we send that processed data to our endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.predict(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see the prediction has sent back a confidence score for each class. The second value in the list corresponds to the class label \"Roundabout\" which has the highest confidence score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using boto3 sagemaker_runtime client\n",
    "\n",
    "So what if we want to make predictions against this endpoint outside of this notebook?<br>\n",
    "We then leverage the boto3 library. <br>\n",
    "**NOTE** Replace **'your-endpoint-name'** with your endpoint name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "client=boto3.client('sagemaker-runtime')\n",
    "response = client.invoke_endpoint(\n",
    "            EndpointName='your-endpoint-name',\n",
    "            ContentType='application/json',\n",
    "            Body=json.dumps({'instances':x.tolist()}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now view the JSON response. Again the second value in the list corresponds to the class label \"Roundabout\" which has the highest confidence score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json.loads(response['Body'].read().decode(\"utf-8\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will start by creating a S3 URI to the model artifacts package generated from the training step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_data = f\"{estimator_tf.output_path}{estimator_tf._current_job_name}/output/model.tar.gz\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will create a local export folder so we can store our inference code in a code folder. We can also specify a requirements.txt for any package dependencies  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! mkdir ./export"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we copy and unpack the model artifacts file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws s3 cp {model_data} ./export/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tar -xvzf ./export/model.tar.gz -C ./export/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd export"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now delete any old model artifacts folders and move the unpacked model artifacts folder to the 1 folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! rm -r 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! mv tf000000001/1 ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! rm -r code/.ipynb_checkpoints/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now package up the code and 1 folder to create a new model.tar.gz file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! tar -czvf model.tar.gz code 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We copy the new model.tar.gz to your S3 bucket and setup our Tensorflow serving container"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sagemaker\n",
    "from sagemaker.tensorflow.model import TensorFlowModel\n",
    "sm_role=sagemaker.get_execution_role()\n",
    "sagemaker_session = sagemaker.Session()\n",
    "# See the following document for more on SageMaker Roles:\n",
    "# https://docs.aws.amazon.com/sagemaker/latest/dg/sagemaker-roles.html\n",
    "role = sm_role\n",
    "\n",
    "# Will be using the bucket variable defined at beginning of this notebook\n",
    "\n",
    "prefix = 'tf_model'\n",
    "s3_path = 's3://{}/{}'.format(bucket, prefix)\n",
    "\n",
    "model_data = sagemaker_session.upload_data('model.tar.gz',\n",
    "                                           bucket,\n",
    "                                           os.path.join(prefix, 'model'))\n",
    "                                           \n",
    "tensorflow_serving_model = TensorFlowModel(model_data=model_data,\n",
    "                                 role=role,\n",
    "                                 framework_version='2.4.1',\n",
    "                                 sagemaker_session=sagemaker_session)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then specify the output folder and run the transformer method to start the batch processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path = f's3://{bucket}/{prefix}/output'\n",
    "tensorflow_serving_transformer = tensorflow_serving_model.transformer(\n",
    "                                     instance_count=2,\n",
    "                                     instance_type='ml.m5.4xlarge',\n",
    "                                     max_concurrent_transforms=64,\n",
    "                                     max_payload=1,\n",
    "                                     output_path=output_path)\n",
    "\n",
    "input_path = f's3://{bucket}/test'\n",
    "tensorflow_serving_transformer.transform(input_path, content_type='application/x-image')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can look at the output file from the batch job. Each file is a prediction that corresponds to the input image file name "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! aws s3 ls {output_path} --recursive | grep -v \".ipy\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "endtime = datetime.datetime.now()\n",
    "str(endtime-sttime)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Appendix and Utilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attach to a training job that has been left to run \n",
    "\n",
    "If your kernel becomes disconnected and your training has already started, you can reattach to the training job.<br>\n",
    "Simply look up the training job name and replace the **your-training-job-name** and then run the cell below. <br>\n",
    "Once the training job is finished, you can continue the cells after the training cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import sagemaker\n",
    "import boto3\n",
    "\n",
    "sess = sagemaker.Session()\n",
    "\n",
    "training_job_name = 'your-training-job-name'\n",
    "\n",
    "estimator_tf = sagemaker.estimator.Estimator.attach(training_job_name=training_job_name, sagemaker_session=sess)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Raj needs to review to see if we still want to keep the content below"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sagemaker 2 update endpoint steps\n",
    "\n",
    "import sagemaker\n",
    "\n",
    "predictor = sagemaker.pytorch.model.PyTorchPredictor(existing_endpoint_name)\n",
    "predictor.update_endpoint(initial_instance_count=1, instance_type=\"ml.p3.2xlarge\", model_name= new_model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ditributed Data Parallel - Code Modification\n",
    "\n",
    "https://docs.aws.amazon.com/sagemaker/latest/dg/data-parallel-modify-sdp-tf2.html\n",
    "\n",
    "https://docs.aws.amazon.com/sagemaker/latest/dg/model-parallel-use-api.html\n",
    "\n",
    "```\n",
    "import tensorflow as tf\n",
    "\n",
    "**SageMaker data parallel: Import the library TF API **\n",
    "import smdistributed.dataparallel.tensorflow as sdp\n",
    "\n",
    "**SageMaker data parallel: Initialize the library**\n",
    "sdp.init()\n",
    "\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "for gpu in gpus:\n",
    "    tf.config.experimental.set_memory_growth(gpu, True)\n",
    "if gpus:\n",
    "    # SageMaker data parallel: Pin GPUs to a single library process\n",
    "    tf.config.experimental.set_visible_devices(gpus[sdp.local_rank()], 'GPU')\n",
    "\n",
    "**Prepare Dataset**\n",
    "dataset = tf.data.Dataset.from_tensor_slices(...)\n",
    "\n",
    "**Define Model**\n",
    "mnist_model = tf.keras.Sequential(...)\n",
    "loss = tf.losses.SparseCategoricalCrossentropy()\n",
    "\n",
    "**SageMaker data parallel: Scale Learning Rate**\n",
    "*LR for 8 node run : 0.000125*\n",
    "*LR for single node run : 0.001*\n",
    "opt = tf.optimizers.Adam(0.000125 * sdp.size())\n",
    "\n",
    "@tf.function\n",
    "def training_step(images, labels, first_batch):\n",
    "    with tf.GradientTape() as tape:\n",
    "        probs = mnist_model(images, training=True)\n",
    "        loss_value = loss(labels, probs)\n",
    "\n",
    "    **SageMaker data parallel: Wrap tf.GradientTape with the library's DistributedGradientTape**\n",
    "    tape = sdp.DistributedGradientTape(tape)\n",
    "\n",
    "    grads = tape.gradient(loss_value, mnist_model.trainable_variables)\n",
    "    opt.apply_gradients(zip(grads, mnist_model.trainable_variables))\n",
    "\n",
    "    if first_batch:\n",
    "       **SageMaker data parallel: Broadcast model and optimizer variables**\n",
    "       sdp.broadcast_variables(mnist_model.variables, root_rank=0)\n",
    "       sdp.broadcast_variables(opt.variables(), root_rank=0)\n",
    "\n",
    "    return loss_value\n",
    "\n",
    "# SageMaker data parallel: Save checkpoints only from master node.\n",
    "if sdp.rank() == 0:\n",
    "    checkpoint.save(checkpoint_dir)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### On estimator side\n",
    "```\n",
    "from sagemaker.tensorflow import TensorFlow\n",
    "\n",
    "tf_estimator = TensorFlow(\n",
    "    base_job_name = \"training_job_name_prefix\",\n",
    "    entry_point=\"tf-train.py\",\n",
    "    role=\"SageMakerRole\",\n",
    "    framework_version=\"2.4.1\",\n",
    "    # You must set py_version to py36\n",
    "    py_version=\"py37\",\n",
    "    # For training with multi node distributed training, set this count.\n",
    "    # Example: 2,3,4,..8\n",
    "    instance_count=2,\n",
    "    # For training with p3dn instance use - ml.p3dn.24xlarge\n",
    "    instance_type=\"ml.p3.16xlarge\",\n",
    "    # Training using smdistributed.dataparallel Distributed Training Framework\n",
    "    distribution={\"smdistributed\": {\"dataparallel\": {\"enabled\": True}}}\n",
    ")\n",
    "\n",
    "tf_estimator.fit(\"s3://bucket/path/to/training/data\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_amazonei_tensorflow2_p36",
   "language": "python",
   "name": "conda_amazonei_tensorflow2_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
