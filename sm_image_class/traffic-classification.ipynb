{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: right\"> &uarr;   Ensure Kernel is set to  &uarr;  </div><br><div style=\"text-align: right\"> \n",
    "conda_mxnet_latest_p37  </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SageMaker Image Classification Built-In Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction \n",
    "The Amazon SageMaker image classification algorithm is a supervised learning algorithm that supports multi-label classification. It takes an image as input and outputs one or more labels assigned to that image. It uses a convolutional neural network (ResNet) that can be trained from scratch or trained using transfer learning when a large number of training images are not available."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The outline of this notebook is \n",
    "\n",
    "1. Prepare images into RecordIO format\n",
    "\n",
    "2. Train the SageMaker Image Classification built-in algorithm \n",
    "\n",
    "3. Create and deploy the model to an endpoint for doing inference \n",
    "\n",
    "4. Test realtime inference with the endpoint\n",
    "\n",
    "5. Do batch inference using SageMaker Batch Transform\n",
    "\n",
    "Lets start by importing some base libraries and some initial variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the cell below, replace **your-unique-bucket-name** with the name of bucket you created in the data-prep notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 802 ms, sys: 226 ms, total: 1.03 s\n",
      "Wall time: 915 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import boto3\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker.amazon.amazon_estimator import get_image_uri\n",
    "\n",
    "role = get_execution_role()\n",
    "\n",
    "sess = sagemaker.Session()\n",
    "\n",
    "bucket = 'your-unique-bucket-name'\n",
    "\n",
    "training_image = sagemaker.image_uris.retrieve(region=boto3.Session().region_name, framework='image-classification')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find mxnet  so we can use some of the tools to create RecordIO format datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "imrec = ! find $CONDA_PREFIX -name im2rec.py | grep -v gpu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now store the location of the MXNet tool im2rec.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "imrec_loc = imrec[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation\n",
    "\n",
    "Lets first list out the folders in our data folder "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test\n",
      "train\n",
      "val\n"
     ]
    }
   ],
   "source": [
    "! ls -1 ../data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we create a folder to store our RecordIO files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘recordio_dataset’: File exists\n"
     ]
    }
   ],
   "source": [
    "! mkdir recordio_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now build our train and validation datasets in recordio format\n",
    "First we generate list files using im2rec.py from mxnet <br>\n",
    "The output will show the class label and its assigned number (implied from the folder structure)<br>\n",
    "i.e.<br>\n",
    "Priority 0<br>\n",
    "Roundabout 1<br>\n",
    "Signal 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Priority 0\n",
      "Roundabout 1\n",
      "Signal 2\n"
     ]
    }
   ],
   "source": [
    "! python {imrec_loc} recordio_dataset/train ../data/train --recursive --list --num-thread 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Priority 0\n",
      "Roundabout 1\n",
      "Signal 2\n"
     ]
    }
   ],
   "source": [
    "! python {imrec_loc} recordio_dataset/validation ../data/val --recursive --list --num-thread 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have generated the list files, we will use them to generate the respective training and validation recordio files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating .rec file from /home/ec2-user/SageMaker/pssummitwkshp/sm_image_class/recordio_dataset/train.lst in /home/ec2-user/SageMaker/pssummitwkshp/sm_image_class/recordio_dataset\n",
      "multiprocessing not available, fall back to single threaded encoding\n",
      "time: 0.014767169952392578  count: 0\n",
      "time: 14.248790740966797  count: 1000\n"
     ]
    }
   ],
   "source": [
    "! python {imrec_loc} recordio_dataset/train.lst ../data/train "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating .rec file from /home/ec2-user/SageMaker/pssummitwkshp/sm_image_class/recordio_dataset/validation.lst in /home/ec2-user/SageMaker/pssummitwkshp/sm_image_class/recordio_dataset\n",
      "multiprocessing not available, fall back to single threaded encoding\n",
      "time: 0.014941930770874023  count: 0\n"
     ]
    }
   ],
   "source": [
    "! python {imrec_loc} recordio_dataset/validation.lst ../data/val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have the train and validation datasets in recordio format, we will now copy them to our S3 bucket "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_train_key = \"recordio_dataset/train\"\n",
    "s3_validation_key = \"recordio_dataset/validation\"\n",
    "\n",
    "s3_train = 's3://{}/{}/'.format(bucket, s3_train_key)\n",
    "s3_validation = 's3://{}/{}/'.format(bucket, s3_validation_key)\n",
    "\n",
    "s3_train_lst = 's3://{}/{}/'.format(bucket, \"recordio_dataset/lst/train.lst\")\n",
    "s3_validation_lst = 's3://{}/{}/'.format(bucket, \"recordio_dataset/lst/validation.lst\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "upload: recordio_dataset/train.rec to s3://del-me-bucket/recordio_dataset/train/train.rec\n",
      "upload: recordio_dataset/train.idx to s3://del-me-bucket/recordio_dataset/train/train.idx\n",
      "upload: recordio_dataset/validation.rec to s3://del-me-bucket/recordio_dataset/validation/validation.rec\n",
      "upload: recordio_dataset/validation.idx to s3://del-me-bucket/recordio_dataset/validation/validation.idx\n",
      "upload: recordio_dataset/train.lst to s3://del-me-bucket/recordio_dataset/lst/train.lst/train.lst\n",
      "upload: recordio_dataset/validation.lst to s3://del-me-bucket/recordio_dataset/lst/validation.lst/validation.lst\n"
     ]
    }
   ],
   "source": [
    "! aws s3 cp recordio_dataset/train.rec {s3_train}\n",
    "! aws s3 cp recordio_dataset/train.idx {s3_train}\n",
    "\n",
    "! aws s3 cp recordio_dataset/validation.rec {s3_validation}\n",
    "! aws s3 cp recordio_dataset/validation.idx {s3_validation}\n",
    "\n",
    "! aws s3 cp recordio_dataset/train.lst {s3_train_lst}\n",
    "! aws s3 cp recordio_dataset/validation.lst {s3_validation_lst}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training parameters\n",
    "There are two kinds of parameters that need to be set for training. The first one are the parameters for the training job. These include:\n",
    "\n",
    "* **Training instance count**: This is the number of instances on which to run the training. When the number of instances is greater than one, then the image classification algorithm will run in distributed settings. \n",
    "* **Training instance type**: This indicates the type of machine on which to run the training. Typically, we use GPU instances for these training \n",
    "* **Output path**: This the s3 folder in which the training output is stored"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_name_prefix = 'traffic-image-classification'\n",
    "job_name = job_name_prefix + '-' + time.strftime('-%Y-%m-%d-%H-%M-%S', time.gmtime())\n",
    "\n",
    "s3_output_location = 's3://{}/{}/output'.format(bucket, job_name_prefix)\n",
    "sm_ic_estimator = sagemaker.estimator.Estimator(\n",
    "    training_image,\n",
    "    role,\n",
    "    instance_count=1,\n",
    "    instance_type=\"ml.p3.2xlarge\",\n",
    "    volume_size=50,\n",
    "    max_run=360000,\n",
    "    input_mode=\"File\",\n",
    "    output_path=s3_output_location,\n",
    "    sagemaker_session=sess,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algorithm parameters\n",
    "\n",
    "Apart from the above set of parameters, there are hyperparameters that are specific to the algorithm. These are:\n",
    "\n",
    "* **num_layers**: The number of layers (depth) for the network. We use 18 in this samples but other values such as 50, 152 can be used.\n",
    "* **use_pretrained_model**: Set to 1 to use pretrained model for transfer learning.\n",
    "* **image_shape**: The input image dimensions,'num_channels, height, width', for the network. It should be no larger than the actual image size. The number of channels should be same as the actual image.\n",
    "* **num_classes**: This is the number of output classes for the dataset. We use 3 classes so we set this value to 3\n",
    "* **mini_batch_size**: The number of training samples used for each mini batch. In distributed training, the number of training samples used per batch will be N * mini_batch_size where N is the number of hosts on which training is run\n",
    "* **resize**: Resize the image before using it for training. The images are resized so that the shortest side is of this parameter. If the parameter is not set, then the training data is used as such without resizing.\n",
    "* **epochs**: Number of training epochs\n",
    "* **learning_rate**: Learning rate for training\n",
    "* **num_training_samples**: This is the total number of training samples. It is set to 1334 for this dataset\n",
    "\n",
    "You can find a detailed description of all the algorithm parameters at https://docs.aws.amazon.com/sagemaker/latest/dg/IC-Hyperparameter.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "sm_ic_estimator.set_hyperparameters(\n",
    "    num_layers=18,\n",
    "    use_pretrained_model=1,\n",
    "    image_shape=\"3,640,640\",\n",
    "    num_classes=3,\n",
    "    mini_batch_size=64,\n",
    "    epochs=50,\n",
    "    learning_rate=0.01,\n",
    "    num_training_samples=1334,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input data specification\n",
    "Set the data type and channels used for training. In this training, we use application/x-recordio content type that require the dataset to be is recordio format and lst file for data input. In addition, Sagemaker image classification algorithm supports application/x-image format "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = sagemaker.inputs.TrainingInput(\n",
    "    s3_train,\n",
    "    distribution=\"FullyReplicated\",\n",
    "    content_type=\"application/x-recordio\",\n",
    "    s3_data_type=\"S3Prefix\",\n",
    ")\n",
    "\n",
    "validation_data = sagemaker.inputs.TrainingInput(\n",
    "    s3_validation,\n",
    "    distribution=\"FullyReplicated\",\n",
    "    content_type=\"application/x-recordio\",\n",
    "    s3_data_type=\"S3Prefix\",\n",
    ")\n",
    "\n",
    "train_data_lst = sagemaker.inputs.TrainingInput(\n",
    "    s3_train_lst,\n",
    "    distribution=\"FullyReplicated\",\n",
    "    content_type=\"text/plain\",\n",
    "    s3_data_type=\"S3Prefix\",\n",
    ")\n",
    "\n",
    "validation_data_lst = sagemaker.inputs.TrainingInput(\n",
    "    s3_validation_lst,\n",
    "    distribution=\"FullyReplicated\",\n",
    "    content_type=\"text/plain\",\n",
    "    s3_data_type=\"S3Prefix\",\n",
    ")\n",
    "\n",
    "data_channels = {\n",
    "    \"train\": train_data,\n",
    "    \"validation\": validation_data,\n",
    "    \"train_lst\": train_data_lst,\n",
    "    \"validation_lst\": validation_data_lst,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now can call the fit method with the input channels on the estimator to start the training<br>\n",
    "**NOTE** This cell takes **16 mins** to run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-05-15 15:15:38 Starting - Starting the training job...\n",
      "2022-05-15 15:16:06 Starting - Preparing the instances for trainingProfilerReport-1652627738: InProgress\n",
      ".........\n",
      "2022-05-15 15:17:29 Downloading - Downloading input data...\n",
      "2022-05-15 15:18:06 Training - Downloading the training image.........\n",
      "2022-05-15 15:19:28 Training - Training image download completed. Training in progress.\u001b[34mDocker entrypoint called with argument(s): train\u001b[0m\n",
      "\u001b[34m[05/15/2022 15:19:35 INFO 140362540005184] Reading default configuration from /opt/amazon/lib/python3.7/site-packages/image_classification/default-input.json: {'use_pretrained_model': 0, 'num_layers': 152, 'epochs': 30, 'learning_rate': 0.1, 'lr_scheduler_factor': 0.1, 'optimizer': 'sgd', 'momentum': 0, 'weight_decay': 0.0001, 'beta_1': 0.9, 'beta_2': 0.999, 'eps': 1e-08, 'gamma': 0.9, 'mini_batch_size': 32, 'image_shape': '3,224,224', 'precision_dtype': 'float32'}\u001b[0m\n",
      "\u001b[34m[05/15/2022 15:19:35 INFO 140362540005184] Merging with provided configuration from /opt/ml/input/config/hyperparameters.json: {'epochs': '50', 'image_shape': '3,640,640', 'learning_rate': '0.01', 'mini_batch_size': '64', 'num_classes': '3', 'num_layers': '18', 'num_training_samples': '1334', 'use_pretrained_model': '1'}\u001b[0m\n",
      "\u001b[34m[05/15/2022 15:19:35 INFO 140362540005184] Final configuration: {'use_pretrained_model': '1', 'num_layers': '18', 'epochs': '50', 'learning_rate': '0.01', 'lr_scheduler_factor': 0.1, 'optimizer': 'sgd', 'momentum': 0, 'weight_decay': 0.0001, 'beta_1': 0.9, 'beta_2': 0.999, 'eps': 1e-08, 'gamma': 0.9, 'mini_batch_size': '64', 'image_shape': '3,640,640', 'precision_dtype': 'float32', 'num_classes': '3', 'num_training_samples': '1334'}\u001b[0m\n",
      "\u001b[34m[05/15/2022 15:19:35 INFO 140362540005184] Searching for .rec files in /opt/ml/input/data/train.\u001b[0m\n",
      "\u001b[34m[05/15/2022 15:19:35 INFO 140362540005184] Searching for .rec files in /opt/ml/input/data/validation.\u001b[0m\n",
      "\u001b[34m[05/15/2022 15:19:35 INFO 140362540005184] use_pretrained_model: 1\u001b[0m\n",
      "\u001b[34m[05/15/2022 15:19:35 INFO 140362540005184] multi_label: 0\u001b[0m\n",
      "\u001b[34m[05/15/2022 15:19:35 INFO 140362540005184] Using pretrained model for initializing weights and transfer learning.\u001b[0m\n",
      "\u001b[34m[05/15/2022 15:19:35 INFO 140362540005184] ---- Parameters ----\u001b[0m\n",
      "\u001b[34m[05/15/2022 15:19:35 INFO 140362540005184] num_layers: 18\u001b[0m\n",
      "\u001b[34m[05/15/2022 15:19:35 INFO 140362540005184] data type: <class 'numpy.float32'>\u001b[0m\n",
      "\u001b[34m[05/15/2022 15:19:35 INFO 140362540005184] epochs: 50\u001b[0m\n",
      "\u001b[34m[05/15/2022 15:19:35 INFO 140362540005184] optimizer: sgd\u001b[0m\n",
      "\u001b[34m[05/15/2022 15:19:35 INFO 140362540005184] momentum: 0.9\u001b[0m\n",
      "\u001b[34m[05/15/2022 15:19:35 INFO 140362540005184] weight_decay: 0.0001\u001b[0m\n",
      "\u001b[34m[05/15/2022 15:19:35 INFO 140362540005184] learning_rate: 0.01\u001b[0m\n",
      "\u001b[34m[05/15/2022 15:19:35 INFO 140362540005184] num_training_samples: 1334\u001b[0m\n",
      "\u001b[34m[05/15/2022 15:19:35 INFO 140362540005184] mini_batch_size: 64\u001b[0m\n",
      "\u001b[34m[05/15/2022 15:19:35 INFO 140362540005184] image_shape: 3,640,640\u001b[0m\n",
      "\u001b[34m[05/15/2022 15:19:35 INFO 140362540005184] num_classes: 3\u001b[0m\n",
      "\u001b[34m[05/15/2022 15:19:35 INFO 140362540005184] augmentation_type: None\u001b[0m\n",
      "\u001b[34m[05/15/2022 15:19:35 INFO 140362540005184] kv_store: device\u001b[0m\n",
      "\u001b[34m[05/15/2022 15:19:35 INFO 140362540005184] checkpoint_frequency not set, will store the best model\u001b[0m\n",
      "\u001b[34m[05/15/2022 15:19:35 INFO 140362540005184] --------------------\u001b[0m\n",
      "\u001b[34m[15:19:35] /opt/brazil-pkg-cache/packages/AIAlgorithmsMXNet/AIAlgorithmsMXNet-1.3.x_ecl_Cuda_10.1.x.11282.0/AL2_x86_64/generic-flavor/src/src/nnvm/legacy_json_util.cc:209: Loading symbol saved by previous version v0.8.0. Attempting to upgrade...\u001b[0m\n",
      "\u001b[34m[15:19:35] /opt/brazil-pkg-cache/packages/AIAlgorithmsMXNet/AIAlgorithmsMXNet-1.3.x_ecl_Cuda_10.1.x.11282.0/AL2_x86_64/generic-flavor/src/src/nnvm/legacy_json_util.cc:217: Symbol successfully upgraded!\u001b[0m\n",
      "\u001b[34m[05/15/2022 15:19:35 INFO 140362540005184] Setting number of threads: 7\u001b[0m\n",
      "\u001b[34m[15:19:47] /opt/brazil-pkg-cache/packages/AIAlgorithmsMXNet/AIAlgorithmsMXNet-1.3.x_ecl_Cuda_10.1.x.11282.0/AL2_x86_64/generic-flavor/src/src/operator/nn/./cudnn/./cudnn_algoreg-inl.h:97: Running performance tests to find the best convolution algorithm, this can take a while... (setting env variable MXNET_CUDNN_AUTOTUNE_DEFAULT to 0 to disable)\u001b[0m\n",
      "\u001b[34m[05/15/2022 15:20:02 INFO 140362540005184] Epoch[0] Train-accuracy=0.553906\u001b[0m\n",
      "\u001b[34m[05/15/2022 15:20:02 INFO 140362540005184] Epoch[0] Time cost=15.023\u001b[0m\n",
      "\u001b[34m[05/15/2022 15:20:04 INFO 140362540005184] Epoch[0] Validation-accuracy=0.609375\u001b[0m\n",
      "\u001b[34m[05/15/2022 15:20:04 INFO 140362540005184] Storing the best model with validation accuracy: 0.609375\u001b[0m\n",
      "\u001b[34m[05/15/2022 15:20:04 INFO 140362540005184] Saved checkpoint to \"/opt/ml/model/image-classification-0001.params\"\u001b[0m\n",
      "\u001b[34m[05/15/2022 15:20:15 INFO 140362540005184] Epoch[1] Train-accuracy=0.792188\u001b[0m\n",
      "\u001b[34m[05/15/2022 15:20:15 INFO 140362540005184] Epoch[1] Time cost=10.860\u001b[0m\n",
      "\u001b[34m[05/15/2022 15:20:16 INFO 140362540005184] Epoch[1] Validation-accuracy=0.671875\u001b[0m\n",
      "\u001b[34m[05/15/2022 15:20:16 INFO 140362540005184] Storing the best model with validation accuracy: 0.671875\u001b[0m\n",
      "\u001b[34m[05/15/2022 15:20:17 INFO 140362540005184] Saved checkpoint to \"/opt/ml/model/image-classification-0002.params\"\u001b[0m\n",
      "\u001b[34m[05/15/2022 15:20:27 INFO 140362540005184] Epoch[2] Train-accuracy=0.900781\u001b[0m\n",
      "\u001b[34m[05/15/2022 15:20:27 INFO 140362540005184] Epoch[2] Time cost=10.784\u001b[0m\n",
      "\u001b[34m[05/15/2022 15:20:29 INFO 140362540005184] Epoch[2] Validation-accuracy=0.740625\u001b[0m\n",
      "\u001b[34m[05/15/2022 15:20:29 INFO 140362540005184] Storing the best model with validation accuracy: 0.740625\u001b[0m\n",
      "\u001b[34m[05/15/2022 15:20:29 INFO 140362540005184] Saved checkpoint to \"/opt/ml/model/image-classification-0003.params\"\u001b[0m\n",
      "\u001b[34m[05/15/2022 15:20:40 INFO 140362540005184] Epoch[3] Train-accuracy=0.960156\u001b[0m\n",
      "\u001b[34m[05/15/2022 15:20:40 INFO 140362540005184] Epoch[3] Time cost=10.825\u001b[0m\n",
      "\u001b[34m[05/15/2022 15:20:42 INFO 140362540005184] Epoch[3] Validation-accuracy=0.742188\u001b[0m\n",
      "\u001b[34m[05/15/2022 15:20:42 INFO 140362540005184] Storing the best model with validation accuracy: 0.742188\u001b[0m\n",
      "\u001b[34m[05/15/2022 15:20:42 INFO 140362540005184] Saved checkpoint to \"/opt/ml/model/image-classification-0004.params\"\u001b[0m\n",
      "\u001b[34m[05/15/2022 15:20:53 INFO 140362540005184] Epoch[4] Train-accuracy=0.971875\u001b[0m\n",
      "\u001b[34m[05/15/2022 15:20:53 INFO 140362540005184] Epoch[4] Time cost=10.673\u001b[0m\n",
      "\u001b[34m[05/15/2022 15:20:54 INFO 140362540005184] Epoch[4] Validation-accuracy=0.762500\u001b[0m\n",
      "\u001b[34m[05/15/2022 15:20:54 INFO 140362540005184] Storing the best model with validation accuracy: 0.762500\u001b[0m\n",
      "\u001b[34m[05/15/2022 15:20:54 INFO 140362540005184] Saved checkpoint to \"/opt/ml/model/image-classification-0005.params\"\u001b[0m\n",
      "\u001b[34m[05/15/2022 15:21:05 INFO 140362540005184] Epoch[5] Train-accuracy=0.979688\u001b[0m\n",
      "\u001b[34m[05/15/2022 15:21:05 INFO 140362540005184] Epoch[5] Time cost=10.855\u001b[0m\n",
      "\u001b[34m[05/15/2022 15:21:07 INFO 140362540005184] Epoch[5] Validation-accuracy=0.725000\u001b[0m\n",
      "\u001b[34m[05/15/2022 15:21:18 INFO 140362540005184] Epoch[6] Train-accuracy=0.987500\u001b[0m\n",
      "\u001b[34m[05/15/2022 15:21:18 INFO 140362540005184] Epoch[6] Time cost=10.840\u001b[0m\n",
      "\u001b[34m[05/15/2022 15:21:19 INFO 140362540005184] Epoch[6] Validation-accuracy=0.740625\u001b[0m\n",
      "\u001b[34m[05/15/2022 15:21:30 INFO 140362540005184] Epoch[7] Train-accuracy=0.990625\u001b[0m\n",
      "\u001b[34m[05/15/2022 15:21:30 INFO 140362540005184] Epoch[7] Time cost=10.745\u001b[0m\n",
      "\u001b[34m[05/15/2022 15:21:32 INFO 140362540005184] Epoch[7] Validation-accuracy=0.822917\u001b[0m\n",
      "\u001b[34m[05/15/2022 15:21:32 INFO 140362540005184] Storing the best model with validation accuracy: 0.822917\u001b[0m\n",
      "\u001b[34m[05/15/2022 15:21:32 INFO 140362540005184] Saved checkpoint to \"/opt/ml/model/image-classification-0008.params\"\u001b[0m\n",
      "\u001b[34m[05/15/2022 15:21:43 INFO 140362540005184] Epoch[8] Train-accuracy=0.996875\u001b[0m\n",
      "\u001b[34m[05/15/2022 15:21:43 INFO 140362540005184] Epoch[8] Time cost=10.978\u001b[0m\n",
      "\u001b[34m[05/15/2022 15:21:45 INFO 140362540005184] Epoch[8] Validation-accuracy=0.793750\u001b[0m\n",
      "\u001b[34m[05/15/2022 15:21:56 INFO 140362540005184] Epoch[9] Train-accuracy=1.000000\u001b[0m\n",
      "\u001b[34m[05/15/2022 15:21:56 INFO 140362540005184] Epoch[9] Time cost=10.931\u001b[0m\n",
      "\u001b[34m[05/15/2022 15:21:57 INFO 140362540005184] Epoch[9] Validation-accuracy=0.796875\u001b[0m\n",
      "\u001b[34m[05/15/2022 15:22:09 INFO 140362540005184] Epoch[10] Train-accuracy=1.000000\u001b[0m\n",
      "\u001b[34m[05/15/2022 15:22:09 INFO 140362540005184] Epoch[10] Time cost=10.892\u001b[0m\n",
      "\u001b[34m[05/15/2022 15:22:10 INFO 140362540005184] Epoch[10] Validation-accuracy=0.803125\u001b[0m\n",
      "\u001b[34m[05/15/2022 15:22:21 INFO 140362540005184] Epoch[11] Train-accuracy=1.000000\u001b[0m\n",
      "\u001b[34m[05/15/2022 15:22:21 INFO 140362540005184] Epoch[11] Time cost=10.916\u001b[0m\n",
      "\u001b[34m[05/15/2022 15:22:23 INFO 140362540005184] Epoch[11] Validation-accuracy=0.807292\u001b[0m\n",
      "\u001b[34m[05/15/2022 15:22:34 INFO 140362540005184] Epoch[12] Train-accuracy=1.000000\u001b[0m\n",
      "\u001b[34m[05/15/2022 15:22:34 INFO 140362540005184] Epoch[12] Time cost=10.909\u001b[0m\n",
      "\u001b[34m[05/15/2022 15:22:36 INFO 140362540005184] Epoch[12] Validation-accuracy=0.803125\u001b[0m\n",
      "\u001b[34m[05/15/2022 15:22:47 INFO 140362540005184] Epoch[13] Train-accuracy=1.000000\u001b[0m\n",
      "\u001b[34m[05/15/2022 15:22:47 INFO 140362540005184] Epoch[13] Time cost=10.786\u001b[0m\n",
      "\u001b[34m[05/15/2022 15:22:48 INFO 140362540005184] Epoch[13] Validation-accuracy=0.803125\u001b[0m\n",
      "\u001b[34m[05/15/2022 15:22:59 INFO 140362540005184] Epoch[14] Train-accuracy=1.000000\u001b[0m\n",
      "\u001b[34m[05/15/2022 15:22:59 INFO 140362540005184] Epoch[14] Time cost=10.981\u001b[0m\n",
      "\u001b[34m[05/15/2022 15:23:01 INFO 140362540005184] Epoch[14] Validation-accuracy=0.806250\u001b[0m\n",
      "\u001b[34m[05/15/2022 15:23:12 INFO 140362540005184] Epoch[15] Train-accuracy=1.000000\u001b[0m\n",
      "\u001b[34m[05/15/2022 15:23:12 INFO 140362540005184] Epoch[15] Time cost=10.956\u001b[0m\n",
      "\u001b[34m[05/15/2022 15:23:14 INFO 140362540005184] Epoch[15] Validation-accuracy=0.806250\u001b[0m\n",
      "\u001b[34m[05/15/2022 15:23:25 INFO 140362540005184] Epoch[16] Train-accuracy=1.000000\u001b[0m\n",
      "\u001b[34m[05/15/2022 15:23:25 INFO 140362540005184] Epoch[16] Time cost=10.886\u001b[0m\n",
      "\u001b[34m[05/15/2022 15:23:27 INFO 140362540005184] Epoch[16] Validation-accuracy=0.807292\u001b[0m\n",
      "\u001b[34m[05/15/2022 15:23:38 INFO 140362540005184] Epoch[17] Train-accuracy=1.000000\u001b[0m\n",
      "\u001b[34m[05/15/2022 15:23:38 INFO 140362540005184] Epoch[17] Time cost=10.770\u001b[0m\n",
      "\u001b[34m[05/15/2022 15:23:40 INFO 140362540005184] Epoch[17] Validation-accuracy=0.800000\u001b[0m\n",
      "\u001b[34m[05/15/2022 15:23:51 INFO 140362540005184] Epoch[18] Train-accuracy=1.000000\u001b[0m\n",
      "\u001b[34m[05/15/2022 15:23:51 INFO 140362540005184] Epoch[18] Time cost=10.890\u001b[0m\n",
      "\u001b[34m[05/15/2022 15:23:52 INFO 140362540005184] Epoch[18] Validation-accuracy=0.803125\u001b[0m\n",
      "\u001b[34m[05/15/2022 15:24:04 INFO 140362540005184] Epoch[19] Train-accuracy=1.000000\u001b[0m\n",
      "\u001b[34m[05/15/2022 15:24:04 INFO 140362540005184] Epoch[19] Time cost=10.917\u001b[0m\n",
      "\u001b[34m[05/15/2022 15:24:05 INFO 140362540005184] Epoch[19] Validation-accuracy=0.803125\u001b[0m\n",
      "\u001b[34m[05/15/2022 15:24:16 INFO 140362540005184] Epoch[20] Train-accuracy=1.000000\u001b[0m\n",
      "\u001b[34m[05/15/2022 15:24:16 INFO 140362540005184] Epoch[20] Time cost=10.725\u001b[0m\n",
      "\u001b[34m[05/15/2022 15:24:18 INFO 140362540005184] Epoch[20] Validation-accuracy=0.812500\u001b[0m\n",
      "\u001b[34m[05/15/2022 15:24:29 INFO 140362540005184] Epoch[21] Train-accuracy=1.000000\u001b[0m\n",
      "\u001b[34m[05/15/2022 15:24:29 INFO 140362540005184] Epoch[21] Time cost=10.831\u001b[0m\n",
      "\u001b[34m[05/15/2022 15:24:31 INFO 140362540005184] Epoch[21] Validation-accuracy=0.803125\u001b[0m\n",
      "\u001b[34m[05/15/2022 15:24:42 INFO 140362540005184] Epoch[22] Train-accuracy=1.000000\u001b[0m\n",
      "\u001b[34m[05/15/2022 15:24:42 INFO 140362540005184] Epoch[22] Time cost=10.820\u001b[0m\n",
      "\u001b[34m[05/15/2022 15:24:43 INFO 140362540005184] Epoch[22] Validation-accuracy=0.803125\u001b[0m\n",
      "\u001b[34m[05/15/2022 15:24:54 INFO 140362540005184] Epoch[23] Train-accuracy=1.000000\u001b[0m\n",
      "\u001b[34m[05/15/2022 15:24:54 INFO 140362540005184] Epoch[23] Time cost=10.751\u001b[0m\n",
      "\u001b[34m[05/15/2022 15:24:56 INFO 140362540005184] Epoch[23] Validation-accuracy=0.812500\u001b[0m\n",
      "\u001b[34m[05/15/2022 15:25:07 INFO 140362540005184] Epoch[24] Train-accuracy=1.000000\u001b[0m\n",
      "\u001b[34m[05/15/2022 15:25:07 INFO 140362540005184] Epoch[24] Time cost=10.846\u001b[0m\n",
      "\u001b[34m[05/15/2022 15:25:09 INFO 140362540005184] Epoch[24] Validation-accuracy=0.815104\u001b[0m\n",
      "\u001b[34m[05/15/2022 15:25:20 INFO 140362540005184] Epoch[25] Train-accuracy=1.000000\u001b[0m\n",
      "\u001b[34m[05/15/2022 15:25:20 INFO 140362540005184] Epoch[25] Time cost=10.831\u001b[0m\n",
      "\u001b[34m[05/15/2022 15:25:21 INFO 140362540005184] Epoch[25] Validation-accuracy=0.809375\u001b[0m\n",
      "\u001b[34m[05/15/2022 15:25:32 INFO 140362540005184] Epoch[26] Train-accuracy=1.000000\u001b[0m\n",
      "\u001b[34m[05/15/2022 15:25:32 INFO 140362540005184] Epoch[26] Time cost=10.721\u001b[0m\n",
      "\u001b[34m[05/15/2022 15:25:34 INFO 140362540005184] Epoch[26] Validation-accuracy=0.809375\u001b[0m\n",
      "\u001b[34m[05/15/2022 15:25:45 INFO 140362540005184] Epoch[27] Train-accuracy=1.000000\u001b[0m\n",
      "\u001b[34m[05/15/2022 15:25:45 INFO 140362540005184] Epoch[27] Time cost=10.854\u001b[0m\n",
      "\u001b[34m[05/15/2022 15:25:46 INFO 140362540005184] Epoch[27] Validation-accuracy=0.812500\u001b[0m\n",
      "\u001b[34m[05/15/2022 15:25:58 INFO 140362540005184] Epoch[28] Train-accuracy=1.000000\u001b[0m\n",
      "\u001b[34m[05/15/2022 15:25:58 INFO 140362540005184] Epoch[28] Time cost=10.856\u001b[0m\n",
      "\u001b[34m[05/15/2022 15:25:59 INFO 140362540005184] Epoch[28] Validation-accuracy=0.812500\u001b[0m\n",
      "\u001b[34m[05/15/2022 15:26:10 INFO 140362540005184] Epoch[29] Train-accuracy=1.000000\u001b[0m\n",
      "\u001b[34m[05/15/2022 15:26:10 INFO 140362540005184] Epoch[29] Time cost=10.901\u001b[0m\n",
      "\u001b[34m[05/15/2022 15:26:12 INFO 140362540005184] Epoch[29] Validation-accuracy=0.815625\u001b[0m\n",
      "\u001b[34m[05/15/2022 15:26:23 INFO 140362540005184] Epoch[30] Train-accuracy=1.000000\u001b[0m\n",
      "\u001b[34m[05/15/2022 15:26:23 INFO 140362540005184] Epoch[30] Time cost=10.780\u001b[0m\n",
      "\u001b[34m[05/15/2022 15:26:24 INFO 140362540005184] Epoch[30] Validation-accuracy=0.809375\u001b[0m\n",
      "\u001b[34m[05/15/2022 15:26:36 INFO 140362540005184] Epoch[31] Train-accuracy=1.000000\u001b[0m\n",
      "\u001b[34m[05/15/2022 15:26:36 INFO 140362540005184] Epoch[31] Time cost=10.893\u001b[0m\n",
      "\u001b[34m[05/15/2022 15:26:37 INFO 140362540005184] Epoch[31] Validation-accuracy=0.809375\u001b[0m\n",
      "\u001b[34m[05/15/2022 15:26:48 INFO 140362540005184] Epoch[32] Train-accuracy=1.000000\u001b[0m\n",
      "\u001b[34m[05/15/2022 15:26:48 INFO 140362540005184] Epoch[32] Time cost=10.882\u001b[0m\n",
      "\u001b[34m[05/15/2022 15:26:50 INFO 140362540005184] Epoch[32] Validation-accuracy=0.809375\u001b[0m\n",
      "\u001b[34m[05/15/2022 15:27:01 INFO 140362540005184] Epoch[33] Train-accuracy=1.000000\u001b[0m\n",
      "\u001b[34m[05/15/2022 15:27:01 INFO 140362540005184] Epoch[33] Time cost=10.871\u001b[0m\n",
      "\u001b[34m[05/15/2022 15:27:03 INFO 140362540005184] Epoch[33] Validation-accuracy=0.817708\u001b[0m\n",
      "\u001b[34m[05/15/2022 15:27:14 INFO 140362540005184] Epoch[34] Train-accuracy=1.000000\u001b[0m\n",
      "\u001b[34m[05/15/2022 15:27:14 INFO 140362540005184] Epoch[34] Time cost=10.946\u001b[0m\n",
      "\u001b[34m[05/15/2022 15:27:15 INFO 140362540005184] Epoch[34] Validation-accuracy=0.806250\u001b[0m\n",
      "\u001b[34m[05/15/2022 15:27:26 INFO 140362540005184] Epoch[35] Train-accuracy=1.000000\u001b[0m\n",
      "\u001b[34m[05/15/2022 15:27:26 INFO 140362540005184] Epoch[35] Time cost=10.882\u001b[0m\n",
      "\u001b[34m[05/15/2022 15:27:28 INFO 140362540005184] Epoch[35] Validation-accuracy=0.809375\u001b[0m\n",
      "\u001b[34m[05/15/2022 15:27:39 INFO 140362540005184] Epoch[36] Train-accuracy=1.000000\u001b[0m\n",
      "\u001b[34m[05/15/2022 15:27:39 INFO 140362540005184] Epoch[36] Time cost=10.723\u001b[0m\n",
      "\u001b[34m[05/15/2022 15:27:40 INFO 140362540005184] Epoch[36] Validation-accuracy=0.815625\u001b[0m\n",
      "\u001b[34m[05/15/2022 15:27:52 INFO 140362540005184] Epoch[37] Train-accuracy=1.000000\u001b[0m\n",
      "\u001b[34m[05/15/2022 15:27:52 INFO 140362540005184] Epoch[37] Time cost=10.885\u001b[0m\n",
      "\u001b[34m[05/15/2022 15:27:53 INFO 140362540005184] Epoch[37] Validation-accuracy=0.822917\u001b[0m\n",
      "\u001b[34m[05/15/2022 15:28:04 INFO 140362540005184] Epoch[38] Train-accuracy=1.000000\u001b[0m\n",
      "\u001b[34m[05/15/2022 15:28:04 INFO 140362540005184] Epoch[38] Time cost=10.827\u001b[0m\n",
      "\u001b[34m[05/15/2022 15:28:06 INFO 140362540005184] Epoch[38] Validation-accuracy=0.812500\u001b[0m\n",
      "\u001b[34m[05/15/2022 15:28:17 INFO 140362540005184] Epoch[39] Train-accuracy=1.000000\u001b[0m\n",
      "\u001b[34m[05/15/2022 15:28:17 INFO 140362540005184] Epoch[39] Time cost=10.697\u001b[0m\n",
      "\u001b[34m[05/15/2022 15:28:18 INFO 140362540005184] Epoch[39] Validation-accuracy=0.803125\u001b[0m\n",
      "\u001b[34m[05/15/2022 15:28:29 INFO 140362540005184] Epoch[40] Train-accuracy=1.000000\u001b[0m\n",
      "\u001b[34m[05/15/2022 15:28:29 INFO 140362540005184] Epoch[40] Time cost=10.907\u001b[0m\n",
      "\u001b[34m[05/15/2022 15:28:31 INFO 140362540005184] Epoch[40] Validation-accuracy=0.806250\u001b[0m\n",
      "\u001b[34m[05/15/2022 15:28:42 INFO 140362540005184] Epoch[41] Train-accuracy=1.000000\u001b[0m\n",
      "\u001b[34m[05/15/2022 15:28:42 INFO 140362540005184] Epoch[41] Time cost=10.867\u001b[0m\n",
      "\u001b[34m[05/15/2022 15:28:44 INFO 140362540005184] Epoch[41] Validation-accuracy=0.817708\u001b[0m\n",
      "\u001b[34m[05/15/2022 15:28:55 INFO 140362540005184] Epoch[42] Train-accuracy=1.000000\u001b[0m\n",
      "\u001b[34m[05/15/2022 15:28:55 INFO 140362540005184] Epoch[42] Time cost=10.927\u001b[0m\n",
      "\u001b[34m[05/15/2022 15:28:56 INFO 140362540005184] Epoch[42] Validation-accuracy=0.806250\u001b[0m\n",
      "\u001b[34m[05/15/2022 15:29:07 INFO 140362540005184] Epoch[43] Train-accuracy=1.000000\u001b[0m\n",
      "\u001b[34m[05/15/2022 15:29:07 INFO 140362540005184] Epoch[43] Time cost=10.862\u001b[0m\n",
      "\u001b[34m[05/15/2022 15:29:09 INFO 140362540005184] Epoch[43] Validation-accuracy=0.809375\u001b[0m\n",
      "\u001b[34m[05/15/2022 15:29:20 INFO 140362540005184] Epoch[44] Train-accuracy=1.000000\u001b[0m\n",
      "\u001b[34m[05/15/2022 15:29:20 INFO 140362540005184] Epoch[44] Time cost=10.842\u001b[0m\n",
      "\u001b[34m[05/15/2022 15:29:22 INFO 140362540005184] Epoch[44] Validation-accuracy=0.809375\u001b[0m\n",
      "\u001b[34m[05/15/2022 15:29:33 INFO 140362540005184] Epoch[45] Train-accuracy=1.000000\u001b[0m\n",
      "\u001b[34m[05/15/2022 15:29:33 INFO 140362540005184] Epoch[45] Time cost=10.727\u001b[0m\n",
      "\u001b[34m[05/15/2022 15:29:34 INFO 140362540005184] Epoch[45] Validation-accuracy=0.812500\u001b[0m\n",
      "\u001b[34m[05/15/2022 15:29:45 INFO 140362540005184] Epoch[46] Train-accuracy=1.000000\u001b[0m\n",
      "\u001b[34m[05/15/2022 15:29:45 INFO 140362540005184] Epoch[46] Time cost=10.861\u001b[0m\n",
      "\u001b[34m[05/15/2022 15:29:47 INFO 140362540005184] Epoch[46] Validation-accuracy=0.815625\u001b[0m\n",
      "\u001b[34m[05/15/2022 15:29:58 INFO 140362540005184] Epoch[47] Train-accuracy=1.000000\u001b[0m\n",
      "\u001b[34m[05/15/2022 15:29:58 INFO 140362540005184] Epoch[47] Time cost=10.862\u001b[0m\n",
      "\u001b[34m[05/15/2022 15:30:00 INFO 140362540005184] Epoch[47] Validation-accuracy=0.809375\u001b[0m\n",
      "\u001b[34m[05/15/2022 15:30:11 INFO 140362540005184] Epoch[48] Train-accuracy=1.000000\u001b[0m\n",
      "\u001b[34m[05/15/2022 15:30:11 INFO 140362540005184] Epoch[48] Time cost=10.876\u001b[0m\n",
      "\u001b[34m[05/15/2022 15:30:12 INFO 140362540005184] Epoch[48] Validation-accuracy=0.809375\u001b[0m\n",
      "\u001b[34m[05/15/2022 15:30:23 INFO 140362540005184] Epoch[49] Train-accuracy=1.000000\u001b[0m\n",
      "\u001b[34m[05/15/2022 15:30:23 INFO 140362540005184] Epoch[49] Time cost=10.790\u001b[0m\n",
      "\u001b[34m[05/15/2022 15:30:25 INFO 140362540005184] Epoch[49] Validation-accuracy=0.809375\u001b[0m\n",
      "\n",
      "2022-05-15 15:31:49 Uploading - Uploading generated training model\n",
      "2022-05-15 15:31:49 Completed - Training job completed\n",
      "Training seconds: 853\n",
      "Billable seconds: 853\n",
      "CPU times: user 1.92 s, sys: 119 ms, total: 2.04 s\n",
      "Wall time: 16min 34s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "sm_ic_estimator.fit(inputs=data_channels, logs=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **NOTE:** <br>\n",
    "If at this point your kernel disconnects from the server (you can tell because the kernel in the top right hand corner will say **No Kernel**),<br>you can reattach to the training job (so you dont to start the training job again).<br>Follow the steps below\n",
    "1. Scoll your notebook to the top and set the kernel to the recommended kernel specified in the top right hand corner of the notebook\n",
    "2. Go to your SageMaker console, Go to Training Jobs and copy the name of the training job you were disconnected from\n",
    "3. Scoll to the bottom of this notebook, paste your training job name to replace the **your-training-job-name** in the cell\n",
    "4. Replace **your-unique-bucket-name** with the name of bucket you created in the data-prep notebook\n",
    "5. Run the edited cell\n",
    "6. Return to this cell and continue executing the rest of this notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference\n",
    "\n",
    "***\n",
    "\n",
    "A trained model does nothing on its own. We now want to use the model to perform inference. For this example, that means predicting the class of the image.<br>Normally you can deploy the created model by using the deploy method in the estimator as shown in the commented section.<br>\n",
    "Since we are going to use a pretrained model we are going to create a sagemaker model using the training container, location of the model URI and serializer.<br>\n",
    "We will then deploy endpoint using the created model. \n",
    "<br>\n",
    "**NOTE** This cell takes **5 mins** to run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------!CPU times: user 354 ms, sys: 33.2 ms, total: 388 ms\n",
      "Wall time: 5min 32s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from datetime import datetime\n",
    "from sagemaker.serializers import IdentitySerializer\n",
    "from sagemaker.model import Model\n",
    "\n",
    "model_data = 's3://ml-materials/sm_image_class/model.tar.gz'\n",
    "# model_data is set to the pretrained model.\n",
    "# uncomment the following line the get the model URI from the training job\n",
    "#model_data = sm_ic_estimator.model_data\n",
    "\n",
    "endpoint_name = f\"sm-image-classification-{datetime.utcnow():%Y-%m-%d-%H%M}\"\n",
    "\n",
    "sm_client = boto3.Session().client(service_name='sagemaker-runtime') \n",
    "\n",
    "sm_model = Model(image_uri=training_image, \n",
    "              model_data=model_data, \n",
    "              role=role)\n",
    "\n",
    "ic_classifier = sm_model.deploy(\n",
    "    initial_instance_count=1,\n",
    "    instance_type=\"ml.m4.xlarge\",\n",
    "    endpoint_name=endpoint_name\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we get the endpoint name and use boto3 to call the endpoint with our test image<br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 17.7 ms, sys: 1.1 ms, total: 18.8 ms\n",
      "Wall time: 772 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.18177206814289093, 0.4389885365962982, 0.37923941016197205]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "import json\n",
    "\n",
    "im_name=\"../data/test/Roundabout/R2.png\"\n",
    "\n",
    "client = boto3.client('sagemaker-runtime')\n",
    "\n",
    "response = client.invoke_endpoint(\n",
    "    EndpointName=endpoint_name,\n",
    "    ContentType='application/x-image',\n",
    "    Body=open(im_name, 'rb').read())\n",
    "\n",
    "json.loads(response['Body'].read().decode(\"utf-8\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean up\n",
    "You can use the following command to delete the endpoint. The endpoint that is created above is persistent and would consume resources till it is deleted.<br>It is good to delete the endpoint when it is not used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ResponseMetadata': {'RequestId': 'f9c12ed4-b9fd-4b55-888e-d90e6f5a6f80',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amzn-requestid': 'f9c12ed4-b9fd-4b55-888e-d90e6f5a6f80',\n",
       "   'content-type': 'application/x-amz-json-1.1',\n",
       "   'content-length': '0',\n",
       "   'date': 'Sun, 15 May 2022 16:02:27 GMT'},\n",
       "  'RetryAttempts': 0}}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sagemaker_client = boto3.client('sagemaker')\n",
    "sagemaker_client.delete_endpoint(EndpointName=endpoint_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch Inference\n",
    "We are going to use SageMaker Batch Transform to run batch inference on the Test dataset provided"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will start by creating a model in SageMaker. In the request, you name the model and describe a primary container.<br>For the primary container, you specify the Docker image that contains inference code, artifacts (from prior training).<br>You can optionally add a custom environment map that the inference code uses when you deploy the model for predictions.<br>\n",
    "In our case the the docker image is provided by SageMaker, so we will provide the model name and the location of the model artifacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 73.7 ms, sys: 2.34 ms, total: 76 ms\n",
      "Wall time: 611 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from datetime import datetime\n",
    "from sagemaker.serializers import IdentitySerializer\n",
    "from sagemaker.model import Model\n",
    "\n",
    "model_data = 's3://ml-materials/sm_image_class/model.tar.gz'\n",
    "# model_data is set to the pretrained model.\n",
    "# uncomment the following line the get the model URI from the training job\n",
    "#model_data = sm_ic_estimator.model_data\n",
    "\n",
    "model_name=\"traffic-full-image-classification-model\" + time.strftime('-%Y-%m-%d-%H-%M-%S', time.gmtime())\n",
    "\n",
    "sm_model = boto3.Session().client(service_name='sagemaker') \n",
    "\n",
    "primary_container = {\n",
    "    'Image': training_image,\n",
    "    'ModelDataUrl': model_data,\n",
    "}\n",
    "\n",
    "create_model_response = sm_model.create_model(\n",
    "    ModelName = model_name,\n",
    "    ExecutionRoleArn = role,\n",
    "    PrimaryContainer = primary_container)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now populate the Transformer class and provide the instance count, instance type, the model we created and the output path for the results "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.transformer import Transformer\n",
    "\n",
    "batch_output_path = f's3://{bucket}/batch_output'\n",
    "\n",
    "transformer = Transformer(model_name=model_name,\n",
    "                          instance_count=1,\n",
    "                          instance_type='ml.m4.xlarge',\n",
    "                          output_path=batch_output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we call the transform method with the input dataset for the batch inference\n",
    "<br>\n",
    "**NOTE** This cell takes **8 mins** to run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".............................................\n",
      "\u001b[34mDocker entrypoint called with argument(s): serve\u001b[0m\n",
      "\u001b[35mDocker entrypoint called with argument(s): serve\u001b[0m\n",
      "\u001b[34m[05/15/2022 16:14:31 INFO 140689414690624] loaded entry point class algorithm.serve.server_config:config_api\u001b[0m\n",
      "\u001b[34m[05/15/2022 16:14:31 INFO 140689414690624] loading entry points\u001b[0m\n",
      "\u001b[34m[05/15/2022 16:14:31 INFO 140689414690624] loaded request iterator application/x-image\u001b[0m\n",
      "\u001b[35m[05/15/2022 16:14:31 INFO 140689414690624] loaded entry point class algorithm.serve.server_config:config_api\u001b[0m\n",
      "\u001b[35m[05/15/2022 16:14:31 INFO 140689414690624] loading entry points\u001b[0m\n",
      "\u001b[35m[05/15/2022 16:14:31 INFO 140689414690624] loaded request iterator application/x-image\u001b[0m\n",
      "\u001b[34m[05/15/2022 16:14:31 INFO 140689414690624] loaded request iterator image/jpeg\u001b[0m\n",
      "\u001b[34m[05/15/2022 16:14:31 INFO 140689414690624] loaded request iterator image/png\u001b[0m\n",
      "\u001b[34m[05/15/2022 16:14:31 INFO 140689414690624] loaded response encoder application/json\u001b[0m\n",
      "\u001b[34m[05/15/2022 16:14:31 INFO 140689414690624] loaded response encoder application/jsonlines\u001b[0m\n",
      "\u001b[34m[05/15/2022 16:14:31 INFO 140689414690624] loaded entry point class algorithm:model\u001b[0m\n",
      "\u001b[34m[05/15/2022 16:14:31 INFO 140689414690624] Number of server workers: 3\u001b[0m\n",
      "\u001b[34m[2022-05-15 16:14:31 +0000] [1] [INFO] Starting gunicorn 20.1.0\u001b[0m\n",
      "\u001b[34m[2022-05-15 16:14:31 +0000] [1] [INFO] Listening at: http://0.0.0.0:8080 (1)\u001b[0m\n",
      "\u001b[34m[2022-05-15 16:14:31 +0000] [1] [INFO] Using worker: sync\u001b[0m\n",
      "\u001b[34m[2022-05-15 16:14:31 +0000] [42] [INFO] Booting worker with pid: 42\u001b[0m\n",
      "\u001b[34m[05/15/2022 16:14:31 INFO 140689414690624] loading model...\u001b[0m\n",
      "\u001b[34m[05/15/2022 16:14:31 WARNING 140689414690624] Requesting context without setting the requested num of gpus. Using 'auto'\u001b[0m\n",
      "\u001b[34m[2022-05-15 16:14:31 +0000] [60] [INFO] Booting worker with pid: 60\u001b[0m\n",
      "\u001b[34m[05/15/2022 16:14:31 INFO 140689414690624] loading model...\u001b[0m\n",
      "\u001b[34m[05/15/2022 16:14:31 WARNING 140689414690624] Requesting context without setting the requested num of gpus. Using 'auto'\u001b[0m\n",
      "\u001b[34m[2022-05-15 16:14:31 +0000] [81] [INFO] Booting worker with pid: 81\u001b[0m\n",
      "\u001b[34m[05/15/2022 16:14:31 INFO 140689414690624] loading model...\u001b[0m\n",
      "\u001b[34m[05/15/2022 16:14:31 WARNING 140689414690624] Requesting context without setting the requested num of gpus. Using 'auto'\u001b[0m\n",
      "\u001b[34m[05/15/2022 16:14:31 INFO 140689414690624] nvidia-smi: took 0.032 seconds to run.\u001b[0m\n",
      "\u001b[35m[05/15/2022 16:14:31 INFO 140689414690624] loaded request iterator image/jpeg\u001b[0m\n",
      "\u001b[35m[05/15/2022 16:14:31 INFO 140689414690624] loaded request iterator image/png\u001b[0m\n",
      "\u001b[35m[05/15/2022 16:14:31 INFO 140689414690624] loaded response encoder application/json\u001b[0m\n",
      "\u001b[35m[05/15/2022 16:14:31 INFO 140689414690624] loaded response encoder application/jsonlines\u001b[0m\n",
      "\u001b[35m[05/15/2022 16:14:31 INFO 140689414690624] loaded entry point class algorithm:model\u001b[0m\n",
      "\u001b[35m[05/15/2022 16:14:31 INFO 140689414690624] Number of server workers: 3\u001b[0m\n",
      "\u001b[35m[2022-05-15 16:14:31 +0000] [1] [INFO] Starting gunicorn 20.1.0\u001b[0m\n",
      "\u001b[35m[2022-05-15 16:14:31 +0000] [1] [INFO] Listening at: http://0.0.0.0:8080 (1)\u001b[0m\n",
      "\u001b[35m[2022-05-15 16:14:31 +0000] [1] [INFO] Using worker: sync\u001b[0m\n",
      "\u001b[35m[2022-05-15 16:14:31 +0000] [42] [INFO] Booting worker with pid: 42\u001b[0m\n",
      "\u001b[35m[05/15/2022 16:14:31 INFO 140689414690624] loading model...\u001b[0m\n",
      "\u001b[35m[05/15/2022 16:14:31 WARNING 140689414690624] Requesting context without setting the requested num of gpus. Using 'auto'\u001b[0m\n",
      "\u001b[35m[2022-05-15 16:14:31 +0000] [60] [INFO] Booting worker with pid: 60\u001b[0m\n",
      "\u001b[35m[05/15/2022 16:14:31 INFO 140689414690624] loading model...\u001b[0m\n",
      "\u001b[35m[05/15/2022 16:14:31 WARNING 140689414690624] Requesting context without setting the requested num of gpus. Using 'auto'\u001b[0m\n",
      "\u001b[35m[2022-05-15 16:14:31 +0000] [81] [INFO] Booting worker with pid: 81\u001b[0m\n",
      "\u001b[35m[05/15/2022 16:14:31 INFO 140689414690624] loading model...\u001b[0m\n",
      "\u001b[35m[05/15/2022 16:14:31 WARNING 140689414690624] Requesting context without setting the requested num of gpus. Using 'auto'\u001b[0m\n",
      "\u001b[35m[05/15/2022 16:14:31 INFO 140689414690624] nvidia-smi: took 0.032 seconds to run.\u001b[0m\n",
      "\u001b[34m[05/15/2022 16:14:31 INFO 140689414690624] nvidia-smi identified 0 GPUs.\u001b[0m\n",
      "\u001b[34m[05/15/2022 16:14:31 INFO 140689414690624] Number of GPUs being used: 0\u001b[0m\n",
      "\u001b[34m[05/15/2022 16:14:31 INFO 140689414690624] Using device: 0 for worker PID: 42\u001b[0m\n",
      "\u001b[34m[05/15/2022 16:14:31 INFO 140689414690624] CPU model is not found.\u001b[0m\n",
      "\u001b[34m[05/15/2022 16:14:31 INFO 140689414690624] Looking for model file in /opt/ml/model\u001b[0m\n",
      "\u001b[34m[05/15/2022 16:14:31 INFO 140689414690624] Loading /opt/ml/model/image-classification-0010.params\u001b[0m\n",
      "\u001b[34m[05/15/2022 16:14:31 INFO 140689414690624] nvidia-smi: took 0.032 seconds to run.\u001b[0m\n",
      "\u001b[34m[05/15/2022 16:14:31 INFO 140689414690624] nvidia-smi identified 0 GPUs.\u001b[0m\n",
      "\u001b[34m[05/15/2022 16:14:31 INFO 140689414690624] Number of GPUs being used: 0\u001b[0m\n",
      "\u001b[34m[05/15/2022 16:14:31 INFO 140689414690624] Using device: 0 for worker PID: 60\u001b[0m\n",
      "\u001b[34m[05/15/2022 16:14:31 INFO 140689414690624] CPU model is not found.\u001b[0m\n",
      "\u001b[34m[05/15/2022 16:14:31 INFO 140689414690624] Looking for model file in /opt/ml/model\u001b[0m\n",
      "\u001b[34m[05/15/2022 16:14:31 INFO 140689414690624] Loading /opt/ml/model/image-classification-0010.params\u001b[0m\n",
      "\u001b[34m[05/15/2022 16:14:31 INFO 140689414690624] nvidia-smi: took 0.034 seconds to run.\u001b[0m\n",
      "\u001b[34m[05/15/2022 16:14:31 INFO 140689414690624] nvidia-smi identified 0 GPUs.\u001b[0m\n",
      "\u001b[34m[05/15/2022 16:14:31 INFO 140689414690624] Number of GPUs being used: 0\u001b[0m\n",
      "\u001b[34m[05/15/2022 16:14:31 INFO 140689414690624] Using device: 0 for worker PID: 81\u001b[0m\n",
      "\u001b[34m[05/15/2022 16:14:31 INFO 140689414690624] CPU model is not found.\u001b[0m\n",
      "\u001b[34m[05/15/2022 16:14:31 INFO 140689414690624] Looking for model file in /opt/ml/model\u001b[0m\n",
      "\u001b[34m[05/15/2022 16:14:31 INFO 140689414690624] Loading /opt/ml/model/image-classification-0010.params\u001b[0m\n",
      "\u001b[35m[05/15/2022 16:14:31 INFO 140689414690624] nvidia-smi identified 0 GPUs.\u001b[0m\n",
      "\u001b[35m[05/15/2022 16:14:31 INFO 140689414690624] Number of GPUs being used: 0\u001b[0m\n",
      "\u001b[35m[05/15/2022 16:14:31 INFO 140689414690624] Using device: 0 for worker PID: 42\u001b[0m\n",
      "\u001b[35m[05/15/2022 16:14:31 INFO 140689414690624] CPU model is not found.\u001b[0m\n",
      "\u001b[35m[05/15/2022 16:14:31 INFO 140689414690624] Looking for model file in /opt/ml/model\u001b[0m\n",
      "\u001b[35m[05/15/2022 16:14:31 INFO 140689414690624] Loading /opt/ml/model/image-classification-0010.params\u001b[0m\n",
      "\u001b[35m[05/15/2022 16:14:31 INFO 140689414690624] nvidia-smi: took 0.032 seconds to run.\u001b[0m\n",
      "\u001b[35m[05/15/2022 16:14:31 INFO 140689414690624] nvidia-smi identified 0 GPUs.\u001b[0m\n",
      "\u001b[35m[05/15/2022 16:14:31 INFO 140689414690624] Number of GPUs being used: 0\u001b[0m\n",
      "\u001b[35m[05/15/2022 16:14:31 INFO 140689414690624] Using device: 0 for worker PID: 60\u001b[0m\n",
      "\u001b[35m[05/15/2022 16:14:31 INFO 140689414690624] CPU model is not found.\u001b[0m\n",
      "\u001b[35m[05/15/2022 16:14:31 INFO 140689414690624] Looking for model file in /opt/ml/model\u001b[0m\n",
      "\u001b[35m[05/15/2022 16:14:31 INFO 140689414690624] Loading /opt/ml/model/image-classification-0010.params\u001b[0m\n",
      "\u001b[35m[05/15/2022 16:14:31 INFO 140689414690624] nvidia-smi: took 0.034 seconds to run.\u001b[0m\n",
      "\u001b[35m[05/15/2022 16:14:31 INFO 140689414690624] nvidia-smi identified 0 GPUs.\u001b[0m\n",
      "\u001b[35m[05/15/2022 16:14:31 INFO 140689414690624] Number of GPUs being used: 0\u001b[0m\n",
      "\u001b[35m[05/15/2022 16:14:31 INFO 140689414690624] Using device: 0 for worker PID: 81\u001b[0m\n",
      "\u001b[35m[05/15/2022 16:14:31 INFO 140689414690624] CPU model is not found.\u001b[0m\n",
      "\u001b[35m[05/15/2022 16:14:31 INFO 140689414690624] Looking for model file in /opt/ml/model\u001b[0m\n",
      "\u001b[35m[05/15/2022 16:14:31 INFO 140689414690624] Loading /opt/ml/model/image-classification-0010.params\u001b[0m\n",
      "\u001b[34m[05/15/2022 16:14:32 INFO 140689414690624] Using Context: cpu(0)\u001b[0m\n",
      "\u001b[34m[05/15/2022 16:14:32 INFO 140689414690624] Using Context: cpu(0)\u001b[0m\n",
      "\u001b[34m[05/15/2022 16:14:32 INFO 140689414690624] Using Context: cpu(0)\u001b[0m\n",
      "\u001b[34m/opt/amazon/lib/python3.7/site-packages/mxnet/module/base_module.py:67: UserWarning: Data provided by label_shapes don't match names specified by label_names ([] vs. ['softmax_label'])\n",
      "  warnings.warn(msg)\u001b[0m\n",
      "\u001b[34m/opt/amazon/lib/python3.7/site-packages/mxnet/module/base_module.py:67: UserWarning: Data provided by label_shapes don't match names specified by label_names ([] vs. ['softmax_label'])\n",
      "  warnings.warn(msg)\u001b[0m\n",
      "\u001b[34m/opt/amazon/lib/python3.7/site-packages/mxnet/module/base_module.py:67: UserWarning: Data provided by label_shapes don't match names specified by label_names ([] vs. ['softmax_label'])\n",
      "  warnings.warn(msg)\u001b[0m\n",
      "\u001b[34m[05/15/2022 16:14:32 INFO 140689414690624] ...model loaded.\u001b[0m\n",
      "\u001b[34m[05/15/2022 16:14:32 INFO 140689414690624] ...model loaded.\u001b[0m\n",
      "\u001b[34m[05/15/2022 16:14:32 INFO 140689414690624] ...model loaded.\u001b[0m\n",
      "\u001b[35m[05/15/2022 16:14:32 INFO 140689414690624] Using Context: cpu(0)\u001b[0m\n",
      "\u001b[35m[05/15/2022 16:14:32 INFO 140689414690624] Using Context: cpu(0)\u001b[0m\n",
      "\u001b[35m[05/15/2022 16:14:32 INFO 140689414690624] Using Context: cpu(0)\u001b[0m\n",
      "\u001b[35m/opt/amazon/lib/python3.7/site-packages/mxnet/module/base_module.py:67: UserWarning: Data provided by label_shapes don't match names specified by label_names ([] vs. ['softmax_label'])\n",
      "  warnings.warn(msg)\u001b[0m\n",
      "\u001b[35m/opt/amazon/lib/python3.7/site-packages/mxnet/module/base_module.py:67: UserWarning: Data provided by label_shapes don't match names specified by label_names ([] vs. ['softmax_label'])\n",
      "  warnings.warn(msg)\u001b[0m\n",
      "\u001b[35m/opt/amazon/lib/python3.7/site-packages/mxnet/module/base_module.py:67: UserWarning: Data provided by label_shapes don't match names specified by label_names ([] vs. ['softmax_label'])\n",
      "  warnings.warn(msg)\u001b[0m\n",
      "\u001b[35m[05/15/2022 16:14:32 INFO 140689414690624] ...model loaded.\u001b[0m\n",
      "\u001b[35m[05/15/2022 16:14:32 INFO 140689414690624] ...model loaded.\u001b[0m\n",
      "\u001b[35m[05/15/2022 16:14:32 INFO 140689414690624] ...model loaded.\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1652631272.247805, \"EndTime\": 1652631273.6139572, \"Dimensions\": {\"Algorithm\": \"ICModel\", \"Host\": \"UNKNOWN\", \"Operation\": \"scoring\"}, \"Metrics\": {\"execution_parameters.count\": {\"sum\": 1.0, \"count\": 1, \"min\": 1, \"max\": 1}}}\u001b[0m\n",
      "\u001b[34m[16:14:34] /opt/brazil-pkg-cache/packages/AIAlgorithmsMXNet/AIAlgorithmsMXNet-1.3.x_ecl_Cuda_10.1.x.11282.0/AL2_x86_64/generic-flavor/src/src/operator/nn/mkldnn/mkldnn_base.cc:74: Allocate 147456 bytes with malloc directly\u001b[0m\n",
      "\u001b[34m[16:14:34] /opt/brazil-pkg-cache/packages/AIAlgorithmsMXNet/AIAlgorithmsMXNet-1.3.x_ecl_Cuda_10.1.x.11282.0/AL2_x86_64/generic-flavor/src/src/operator/nn/mkldnn/mkldnn_base.cc:74: Allocate 147456 bytes with malloc directly\u001b[0m\n",
      "\u001b[35m#metrics {\"StartTime\": 1652631272.247805, \"EndTime\": 1652631273.6139572, \"Dimensions\": {\"Algorithm\": \"ICModel\", \"Host\": \"UNKNOWN\", \"Operation\": \"scoring\"}, \"Metrics\": {\"execution_parameters.count\": {\"sum\": 1.0, \"count\": 1, \"min\": 1, \"max\": 1}}}\u001b[0m\n",
      "\u001b[35m[16:14:34] /opt/brazil-pkg-cache/packages/AIAlgorithmsMXNet/AIAlgorithmsMXNet-1.3.x_ecl_Cuda_10.1.x.11282.0/AL2_x86_64/generic-flavor/src/src/operator/nn/mkldnn/mkldnn_base.cc:74: Allocate 147456 bytes with malloc directly\u001b[0m\n",
      "\u001b[35m[16:14:34] /opt/brazil-pkg-cache/packages/AIAlgorithmsMXNet/AIAlgorithmsMXNet-1.3.x_ecl_Cuda_10.1.x.11282.0/AL2_x86_64/generic-flavor/src/src/operator/nn/mkldnn/mkldnn_base.cc:74: Allocate 147456 bytes with malloc directly\u001b[0m\n",
      "\u001b[34m[16:14:34] /opt/brazil-pkg-cache/packages/AIAlgorithmsMXNet/AIAlgorithmsMXNet-1.3.x_ecl_Cuda_10.1.x.11282.0/AL2_x86_64/generic-flavor/src/src/operator/nn/mkldnn/mkldnn_base.cc:74: Allocate 147456 bytes with malloc directly\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1652631272.2235613, \"EndTime\": 1652631275.2176974, \"Dimensions\": {\"Algorithm\": \"ICModel\", \"Host\": \"UNKNOWN\", \"Operation\": \"scoring\"}, \"Metrics\": {\"invocations.count\": {\"sum\": 1.0, \"count\": 1, \"min\": 1, \"max\": 1}}}\u001b[0m\n",
      "\u001b[35m[16:14:34] /opt/brazil-pkg-cache/packages/AIAlgorithmsMXNet/AIAlgorithmsMXNet-1.3.x_ecl_Cuda_10.1.x.11282.0/AL2_x86_64/generic-flavor/src/src/operator/nn/mkldnn/mkldnn_base.cc:74: Allocate 147456 bytes with malloc directly\u001b[0m\n",
      "\u001b[35m#metrics {\"StartTime\": 1652631272.2235613, \"EndTime\": 1652631275.2176974, \"Dimensions\": {\"Algorithm\": \"ICModel\", \"Host\": \"UNKNOWN\", \"Operation\": \"scoring\"}, \"Metrics\": {\"invocations.count\": {\"sum\": 1.0, \"count\": 1, \"min\": 1, \"max\": 1}}}\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1652631273.614065, \"EndTime\": 1652631275.837769, \"Dimensions\": {\"Algorithm\": \"ICModel\", \"Host\": \"UNKNOWN\", \"Operation\": \"scoring\"}, \"Metrics\": {\"invocations.count\": {\"sum\": 1.0, \"count\": 1, \"min\": 1, \"max\": 1}}}\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1652631272.25692, \"EndTime\": 1652631275.915835, \"Dimensions\": {\"Algorithm\": \"ICModel\", \"Host\": \"UNKNOWN\", \"Operation\": \"scoring\"}, \"Metrics\": {\"invocations.count\": {\"sum\": 1.0, \"count\": 1, \"min\": 1, \"max\": 1}}}\u001b[0m\n",
      "\u001b[35m#metrics {\"StartTime\": 1652631273.614065, \"EndTime\": 1652631275.837769, \"Dimensions\": {\"Algorithm\": \"ICModel\", \"Host\": \"UNKNOWN\", \"Operation\": \"scoring\"}, \"Metrics\": {\"invocations.count\": {\"sum\": 1.0, \"count\": 1, \"min\": 1, \"max\": 1}}}\u001b[0m\n",
      "\u001b[35m#metrics {\"StartTime\": 1652631272.25692, \"EndTime\": 1652631275.915835, \"Dimensions\": {\"Algorithm\": \"ICModel\", \"Host\": \"UNKNOWN\", \"Operation\": \"scoring\"}, \"Metrics\": {\"invocations.count\": {\"sum\": 1.0, \"count\": 1, \"min\": 1, \"max\": 1}}}\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1652631275.217805, \"EndTime\": 1652631276.6875787, \"Dimensions\": {\"Algorithm\": \"ICModel\", \"Host\": \"UNKNOWN\", \"Operation\": \"scoring\"}, \"Metrics\": {\"invocations.count\": {\"sum\": 1.0, \"count\": 1, \"min\": 1, \"max\": 1}}}\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1652631275.837868, \"EndTime\": 1652631277.214193, \"Dimensions\": {\"Algorithm\": \"ICModel\", \"Host\": \"UNKNOWN\", \"Operation\": \"scoring\"}, \"Metrics\": {\"invocations.count\": {\"sum\": 1.0, \"count\": 1, \"min\": 1, \"max\": 1}}}\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1652631275.9159942, \"EndTime\": 1652631277.3462834, \"Dimensions\": {\"Algorithm\": \"ICModel\", \"Host\": \"UNKNOWN\", \"Operation\": \"scoring\"}, \"Metrics\": {\"invocations.count\": {\"sum\": 1.0, \"count\": 1, \"min\": 1, \"max\": 1}}}\u001b[0m\n",
      "\u001b[35m#metrics {\"StartTime\": 1652631275.217805, \"EndTime\": 1652631276.6875787, \"Dimensions\": {\"Algorithm\": \"ICModel\", \"Host\": \"UNKNOWN\", \"Operation\": \"scoring\"}, \"Metrics\": {\"invocations.count\": {\"sum\": 1.0, \"count\": 1, \"min\": 1, \"max\": 1}}}\u001b[0m\n",
      "\u001b[35m#metrics {\"StartTime\": 1652631275.837868, \"EndTime\": 1652631277.214193, \"Dimensions\": {\"Algorithm\": \"ICModel\", \"Host\": \"UNKNOWN\", \"Operation\": \"scoring\"}, \"Metrics\": {\"invocations.count\": {\"sum\": 1.0, \"count\": 1, \"min\": 1, \"max\": 1}}}\u001b[0m\n",
      "\u001b[35m#metrics {\"StartTime\": 1652631275.9159942, \"EndTime\": 1652631277.3462834, \"Dimensions\": {\"Algorithm\": \"ICModel\", \"Host\": \"UNKNOWN\", \"Operation\": \"scoring\"}, \"Metrics\": {\"invocations.count\": {\"sum\": 1.0, \"count\": 1, \"min\": 1, \"max\": 1}}}\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1652631276.6876552, \"EndTime\": 1652631278.0833762, \"Dimensions\": {\"Algorithm\": \"ICModel\", \"Host\": \"UNKNOWN\", \"Operation\": \"scoring\"}, \"Metrics\": {\"invocations.count\": {\"sum\": 1.0, \"count\": 1, \"min\": 1, \"max\": 1}}}\u001b[0m\n",
      "\u001b[35m#metrics {\"StartTime\": 1652631276.6876552, \"EndTime\": 1652631278.0833762, \"Dimensions\": {\"Algorithm\": \"ICModel\", \"Host\": \"UNKNOWN\", \"Operation\": \"scoring\"}, \"Metrics\": {\"invocations.count\": {\"sum\": 1.0, \"count\": 1, \"min\": 1, \"max\": 1}}}\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1652631277.2142951, \"EndTime\": 1652631278.4321408, \"Dimensions\": {\"Algorithm\": \"ICModel\", \"Host\": \"UNKNOWN\", \"Operation\": \"scoring\"}, \"Metrics\": {\"invocations.count\": {\"sum\": 1.0, \"count\": 1, \"min\": 1, \"max\": 1}}}\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1652631277.346513, \"EndTime\": 1652631278.4721923, \"Dimensions\": {\"Algorithm\": \"ICModel\", \"Host\": \"UNKNOWN\", \"Operation\": \"scoring\"}, \"Metrics\": {\"invocations.count\": {\"sum\": 1.0, \"count\": 1, \"min\": 1, \"max\": 1}}}\u001b[0m\n",
      "\u001b[35m#metrics {\"StartTime\": 1652631277.2142951, \"EndTime\": 1652631278.4321408, \"Dimensions\": {\"Algorithm\": \"ICModel\", \"Host\": \"UNKNOWN\", \"Operation\": \"scoring\"}, \"Metrics\": {\"invocations.count\": {\"sum\": 1.0, \"count\": 1, \"min\": 1, \"max\": 1}}}\u001b[0m\n",
      "\u001b[35m#metrics {\"StartTime\": 1652631277.346513, \"EndTime\": 1652631278.4721923, \"Dimensions\": {\"Algorithm\": \"ICModel\", \"Host\": \"UNKNOWN\", \"Operation\": \"scoring\"}, \"Metrics\": {\"invocations.count\": {\"sum\": 1.0, \"count\": 1, \"min\": 1, \"max\": 1}}}\u001b[0m\n",
      "\u001b[32m2022-05-15T16:14:33.624:[sagemaker logs]: MaxConcurrentTransforms=3, MaxPayloadInMB=6, BatchStrategy=SINGLE_RECORD\u001b[0m\n",
      "CPU times: user 828 ms, sys: 41.1 ms, total: 869 ms\n",
      "Wall time: 7min 44s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "transformer.transform(f's3://{bucket}/test/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Viewing the results of the batch inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "download: s3://del-me-bucket/batch_output/Roundabout/R1.png.out to batch_output/Roundabout/R1.png.out\n",
      "download: s3://del-me-bucket/batch_output/Roundabout/R2.png.out to batch_output/Roundabout/R2.png.out\n",
      "download: s3://del-me-bucket/batch_output/Signal/S1.png.out to batch_output/Signal/S1.png.out\n",
      "download: s3://del-me-bucket/batch_output/Signal/S2.png.out to batch_output/Signal/S2.png.out\n",
      "download: s3://del-me-bucket/batch_output/Priority/P1.png.out to batch_output/Priority/P1.png.out\n",
      "download: s3://del-me-bucket/batch_output/Signal/X1.png.out to batch_output/Signal/X1.png.out\n",
      "download: s3://del-me-bucket/batch_output/Priority/P2.png.out to batch_output/Priority/P2.png.out\n",
      "download: s3://del-me-bucket/batch_output/Roundabout/R3.png.out to batch_output/Roundabout/R3.png.out\n",
      "download: s3://del-me-bucket/batch_output/Roundabout/R1.jpg.out to batch_output/Roundabout/R1.jpg.out\n"
     ]
    }
   ],
   "source": [
    "! aws s3 sync {batch_output_path} batch_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should see a **batch_output** folder. Feed free to navigate and doubleclick the result **.out** files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attach to a training job that has been left to run \n",
    "\n",
    "If your kernel becomes disconnected and your training has already started, you can reattach to the training job.<br>\n",
    "In the cell below, replace **your-unique-bucket-name** with the name of bucket you created in the data-prep notebook<br>\n",
    "Simply look up the training job name and replace the **your-training-job-name** and then run the cell below. <br>\n",
    "Once the training job is finished, you can continue the cells after the training cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "import boto3\n",
    "\n",
    "sess = sagemaker.Session()\n",
    "role = sagemaker.get_execution_role()\n",
    "\n",
    "bucket = \"your-unique-bucket-name\"\n",
    "\n",
    "training_job_name = 'your-training-job-name'\n",
    "\n",
    "if 'your-training' not in training_job_name:\n",
    "    sm_ic_estimator = sagemaker.estimator.Estimator.attach(training_job_name=training_job_name, sagemaker_session=sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "conda_mxnet_latest_p37",
   "language": "python",
   "name": "conda_mxnet_latest_p37"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
